# üöÄ LLM Guardrail Inference Pipeline

*A lightweight, production-ready backend for secure LLM inference, fully open-source.*

This project implements a modern LLM backend featuring:

* üß† **LLM text generation** using HuggingFace Transformers
* üîê **Safety guardrails**: PII detection, harmful content filtering, hallucination heuristics
* üì¶ **Structured outputs** with Pydantic v2
* ü§ñ **Agent tool-calling** (‚ÄúMCP-style‚Äù) ‚Äî includes a real working UTC-time tool
* üóÑÔ∏è **Persistent interaction logging** via SQLModel + SQLite
* ‚ö° **FastAPI REST API** with full Swagger / OpenAPI documentation
* üîß **Configurable model loading** (TinyLlama, Phi-2, Phi-1.5 Mini, etc.)

This serves as a **lightweight open-source alternative** to:
GuardrailsAI ‚Ä¢ OpenAI Moderation ‚Ä¢ Anthropic Constitutional AI
‚Ä¶but implemented fully from scratch for transparency, learning, and extensibility.

---

## ‚ú® Key Features

### üß† 1. HuggingFace LLM Text Generation

* Supports any open HuggingFace causal model
* TinyLlama (default), Phi-2, Phi-1.5 Mini, GPT-like endpoints
* Clean generation pipeline with temperature, top-k, max tokens

### üîê 2. Custom Guardrail System

Includes:

* PII detectors (regex-based)
* Unsafe-content filters
* Hallucination heuristics (unverifiable citations, fake references)
* Unified `GuardrailFlags` schema for downstream consumption

### ü§ñ 3. Agent Tool-Calling (‚ÄúMCP Style‚Äù)

* Pluggable tools
* Example: UTC Time Lookup
* Framework ready for additional tools (web search, DB query, file ops, etc.)

### üóÑÔ∏è 4. Persistent Logging

* Uses **SQLModel + SQLite**
* Stores prompts, responses, guardrail triggers, timestamps, and model metadata
* Perfect for dashboards, RLHF datasets, or audits

### ‚ö° 5. FastAPI Backend with Auto Docs

Once running:

* **Swagger UI:**
  üëâ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

* **OpenAPI schema:**
  üëâ [http://127.0.0.1:8000/openapi.json](http://127.0.0.1:8000/openapi.json)

---

## ‚öôÔ∏è Installation Guide

### 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/YOUR_USERNAME/LLM-Guardrail-Pipeline.git
cd LLM-Guardrail-Pipeline
```

### 2Ô∏è‚É£ Create & activate a virtual environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

**Optional: Enable GPU/MPS acceleration**

```bash
pip install "accelerate>=0.26.0"
```

---

## ‚ñ∂Ô∏è Running the Server

Start the API server:

```bash
uvicorn app.main:app --reload
```

Server will be live at:
üëâ **[http://127.0.0.1:8000](http://127.0.0.1:8000)**

---

## üß© Project Structure

```
app/
 ‚îú‚îÄ‚îÄ main.py               # FastAPI entry point
 ‚îú‚îÄ‚îÄ pipelines.py          # LLM generation + guardrail integration
 ‚îú‚îÄ‚îÄ guardrails.py         # Safety filtering, PII, hallucinations
 ‚îú‚îÄ‚îÄ schemas.py            # Pydantic models (requests/responses)
 ‚îú‚îÄ‚îÄ mcp_agent.py          # Tool-calling agent
 ‚îî‚îÄ‚îÄ logging_utils.py      # SQLModel/SQLite persistence
```

---

# üê≥ **Deploying with Docker (Production-Ready)**

This project ships with a **production-grade Docker image** supporting FastAPI, SQLModel, HuggingFace Transformers, and optional MPS acceleration (Mac M1/M2/M3).
The container is optimized using:

* Slim Python 3.12 base
* Multi-stage caching
* No pip cache
* `.dockerignore` to reduce image bloat
* Uvicorn with multiple workers for production

---

## **1. Build the Docker Image**

From the project root:

```bash
docker build -t llm-guardrails .
```

This will:

* Install system dependencies
* Install Python dependencies
* Copy your application code
* Expose port 8000
* Configure the production Uvicorn entrypoint

---

## **2. Run the Container**

You must pass your `.env` file so the API can load your model:

```bash
docker run -p 8000:8000 --env-file .env llm-guardrails
```

Now your API is live at:

* **[http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)** ‚Üí Swagger UI
* **[http://127.0.0.1:8000/openapi.json](http://127.0.0.1:8000/openapi.json)** ‚Üí OpenAPI schema

---

## **3. Using the API in Production**

A typical request:

```json
POST /generate
{
  "prompt": "Explain reinforcement learning in one sentence.",
  "max_new_tokens": 60,
  "temperature": 0.7
}
```

You can call it from curl:

```bash
curl -X POST http://127.0.0.1:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain RL in one sentence."}'
```

---

## **4. Local Development with Hot Reload (docker-compose)**

If you want the server to auto-reload on code changes:

Create `docker-compose.yaml`:

```yaml
version: "3.9"

services:
  api:
    build: .
    container_name: llm_guardrails
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - .:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Run:

```bash
docker compose up
```

Now you can iterate locally while still inside Docker.

---

## **5. Dockerfile (Included)**

A clean, optimized production Dockerfile is included:

```dockerfile
FROM python:3.12-slim AS base

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1

WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential \
        git \
        curl \
        && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --upgrade pip setuptools wheel && \
    pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

---

## **6. `.dockerignore` (Included)**

This keeps your image small and prevents leaking secrets:

```
.venv/
__pycache__/
*.pyc
*.pyo
*.pyd
*.db
.env
.git/
.gitignore
.cache/
.idea/
.DS_Store
models/
huggingface/
```

---

## **7. Notes for Mac M1/M2/M3 Users (MPS Acceleration)**

Macs with Apple Silicon can run HuggingFace models with hardware acceleration via MPS.

To enable (optional):

Modify your requirements:

```
pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu
```

Transformers will pick up MPS automatically inside or outside Docker.

---

## üåü Why This Project Matters

This repository demonstrates real-world skills involved in AI engineering:

* Building **production-grade LLM APIs**
* Implementing **custom safety + guardrail systems**
* Using **FastAPI, SQLModel, Pydantic v2, and HuggingFace**
* Designing **modular and extensible AI systems**
* Supporting **agent tool-calling patterns similar to MCP**
